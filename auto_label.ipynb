{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# use your own key\n",
    "api_key = \"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\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "model_id = \"gpt-4-1106-preview\"\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\ee527\\Desktop\\Big Data Analytics\\final_project\\podcast_transcript\\5\\test.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    file_contents = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = file_contents.split(\".\")\n",
    "\n",
    "doc = nlp(file_contents)\n",
    "sentences = []\n",
    "for sentence in doc.sents:\n",
    "    sentences.append(sentence.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt engineer here, need to change here \n",
    "number = 5\n",
    "messages = [\n",
    "    \n",
    "        {\"role\": \"user\", \"content\": f\"Read this transcript first: [{file_contents}]. Then give me {number} questions and the complete reference sentence you find in the transcript. Remember, there are some advertisement inside it, please recognize them and dont use those information to ask question. Remember, musn't abbreviate or paraphrase the reference sentence, the refernce sentence must be completely same in the transcript. Lastly, the response should be this format: Question: Who is Donald? Reference: I dont know. Question: Where does he live? Reference: He lives in NYC\"}\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial gpt and start inference\n",
    "# dont need to change here\n",
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=messages,\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test question or answer\n",
    "\n",
    "\n",
    "# post-processing function, if there are quotation marks then remove them\n",
    "def remove_outer_quotes(input_str):\n",
    "    if (input_str.startswith('\"') and input_str.endswith('\"')) or (input_str.startswith(\"'\") and input_str.endswith(\"'\")):\n",
    "        return input_str[1:-1]\n",
    "    else:\n",
    "        return input_str\n",
    "    \n",
    "\n",
    "lines = [line.strip() for line in response.split(\"\\n\") if line.strip()]\n",
    "\n",
    "\n",
    "questions_and_answers = []\n",
    "for line in lines:\n",
    "  \n",
    "\n",
    "    if line[-1] == \"?\":\n",
    "            print(\"q:\")\n",
    "            \n",
    "            colon_index = line.index(':')\n",
    "\n",
    "            # Extract the sentence after the first colon\n",
    "            question = line[colon_index + 1:].strip()\n",
    "            print(question)\n",
    "            \n",
    "            \n",
    "    elif \"Reference\" in line:\n",
    "        print(\"a:\")\n",
    "        \n",
    "        colon_index = line.index(':')\n",
    "\n",
    "        # Extract the sentence after the first colon\n",
    "        answer = line[colon_index + 1:].strip()\n",
    "        answer = remove_outer_quotes(answer)\n",
    "        answer = answer[:-1]\n",
    "        print(answer)\n",
    "        questions_and_answers.append((question, answer))\n",
    "        \n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence matching\n",
    "\n",
    "for qa in questions_and_answers:\n",
    "    \n",
    "        question, answer = qa\n",
    "        \n",
    "        print(\"gpt4 question: \", question)\n",
    "        print(\"gpt4 answer: \", answer)\n",
    "        \n",
    "        ans_start = file_contents.find(answer)\n",
    "        \n",
    "        # first word use lowercase\n",
    "        if ans_start == -1:\n",
    "          \n",
    "          lowercase_answer = answer[0].lower() + answer[1:]\n",
    "          ans_start = file_contents.find(lowercase_answer)\n",
    "          \n",
    "          \n",
    "          # determine by sentence similarity (spacy)\n",
    "          if ans_start == -1:\n",
    "              \n",
    "              similarities = []\n",
    "              \n",
    "              target_doc = nlp(answer)\n",
    "              for sentence in sentences:\n",
    "                  \n",
    "                  doc = nlp(sentence)\n",
    "                  similarity = doc.similarity(target_doc)\n",
    "                  similarities.append(similarity)\n",
    "              \n",
    "              max_similarity = max(similarities)\n",
    "              max_similarity_index = similarities.index(max_similarity)\n",
    "              \n",
    "              \n",
    "              print(\"similarity: \", max(similarities))\n",
    "              print(\"transcript: \", sentences[max_similarity_index])\n",
    "              \n",
    "              if max_similarity >= 0.8:\n",
    "                  \n",
    "                  ans_start = file_contents.find(sentences[max_similarity_index])\n",
    "                  \n",
    "              else:\n",
    "                  print(\"Skip this error gpt response\")\n",
    "                  ans_start = -1\n",
    "              \n",
    "              \n",
    "              \n",
    "          else:\n",
    "              print(\"Find! The first word in transcript is lowercase.\")\n",
    "              \n",
    "        else:\n",
    "            print(\"Find! Completely match.\")\n",
    "              \n",
    "              \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "your_sentence = \"Well, it's been a truly historic week as a Manhattan Grand jury voted to indict former President Donald Trump.\"\n",
    "\n",
    "sentences = [\n",
    "    \"Natural language processing (NLP) is a field of computer science, artificial intelligence.\",\n",
    "    \"NLP techniques are used to analyze, understand, and generate human language in a valuable way.\",\n",
    "    \"It focuses on the interaction between computers and humans through natural language.\",\n",
    "    \"Well, it's been a truly historic week as a Manhattan Grand jury voted to in Diteformer President Donald Trump.\"\n",
    "  \n",
    "]\n",
    "\n",
    "\n",
    "target_doc = nlp(your_sentence)\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    similarity = doc.similarity(target_doc)\n",
    "    print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "path = r\"C:\\Users\\ee527\\Desktop\\Big Data Analytics\\final_project\\podcast_transcript\\4\\5 million In damages.txt\"\n",
    "\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "aaa = text.split(\".\")\n",
    "print(len(aaa))\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "sentence_list = []\n",
    "for sentence in doc.sents:\n",
    "   \n",
    "    sentence_list.append(sentence.text)\n",
    "print(len(sentence_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_list[0])\n",
    "\n",
    "your_sentence = \"Hello and welcome.\"\n",
    "\n",
    "\n",
    "target_doc = nlp(your_sentence)\n",
    "doc = nlp(sentence_list[0])\n",
    "similarity = doc.similarity(target_doc)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
